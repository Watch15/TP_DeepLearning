# ğŸ§  DÃ©tection de commentaires toxiques (TP NLP)

Ce projet est rÃ©alisÃ© dans le cadre dâ€™un TP en groupe sur le **traitement automatique du langage (NLP)** avec une interface Streamlit. Lâ€™objectif est de **classifier des commentaires en ligne toxiques** Ã  partir du dataset **[Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data)** de Kaggle.

---

## ğŸ“¦ Dataset

- ğŸ“ Source : [Kaggle - Jigsaw Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)
- ğŸ“ TÃ¢che : classification **multilabel** (un commentaire peut appartenir Ã  plusieurs catÃ©gories)
- ğŸ“Š Labels :  
  - `toxic`  
  - `severe_toxic`  
  - `obscene`  
  - `threat`  
  - `insult`  
  - `identity_hate`

---

## ğŸ§ª ModÃ¨les utilisÃ©s

### 1. `TF-IDF + LogisticRegression`
- ModÃ¨le de base rapide et lÃ©ger
- EntraÃ®nÃ© avec `OneVsRestClassifier`
- âœ¨ Avantage : temps de prÃ©diction trÃ¨s court

### 2. `DistilBERT`
- ModÃ¨le de Transfert Learning lÃ©ger
- PrÃ©-entraÃ®nÃ© par HuggingFace
- Fine-tuning complet avec PyTorch + HuggingFace `Trainer`
- âš–ï¸ Excellent compromis entre vitesse et performance

### 3. `BERT base`
- ModÃ¨le prÃ©-entraÃ®nÃ© `bert-base-uncased`
- Fine-tuning complet
- ğŸ§  TrÃ¨s performant, mais plus lourd Ã  exÃ©cuter

---

## ğŸš€ Application Streamlit

### FonctionnalitÃ©s :
- Interface interactive pour saisir un commentaire
- PrÃ©diction en temps rÃ©el avec le modÃ¨le sÃ©lectionnÃ©
- Visualisation des probabilitÃ©s par classe
- Comparaison graphique des performances (F1-micro / F1-macro)

### Lancement local :

```bash
streamlit run app.py
![alt text](image.png)