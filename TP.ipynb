{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4600f9f",
   "metadata": {},
   "source": [
    "# TP NLP - Classification de commentaires toxiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d166ac83",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Ce TP a pour objectif de mettre en œuvre des techniques de traitement automatique du langage naturel (**NLP**) pour classifier des commentaires en ligne selon leur niveau de toxicité.\n",
    "\n",
    "Nous utilisons le dataset **Toxic Comment Classification Challenge**, issu de la plateforme [Kaggle](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge). Ce jeu de données contient plus de 150 000 commentaires anonymes postés sur Wikipédia, annotés selon 6 types de toxicité possibles.\n",
    "\n",
    "### Objectifs pédagogiques :\n",
    "- Réaliser une **analyse exploratoire** des données\n",
    "- Appliquer des méthodes de **prétraitement et de transformation** de texte\n",
    "- Entraîner et comparer **trois modèles** de classification multilabel :\n",
    "  - Un modèle de base (TF-IDF + Logistic Regression)\n",
    "  - Deux modèles à base de **Transfert Learning** avec des architectures préentraînées : `DistilBERT` et `BERT`\n",
    "- Développer une **application Streamlit** pour explorer les données et interagir avec les modèles\n",
    "\n",
    "### Tâche NLP visée :\n",
    "Il s'agit d'un problème de **classification multilabel** :\n",
    "un même commentaire peut être à la fois `toxic`, `insult`, et `obscene`, ou au contraire être totalement neutre.\n",
    "\n",
    "### Classes cibles :\n",
    "- `toxic`\n",
    "- `severe_toxic`\n",
    "- `obscene`\n",
    "- `threat`\n",
    "- `insult`\n",
    "- `identity_hate`\n",
    "\n",
    "L’objectif final est de comparer les performances des différents modèles et d’intégrer les meilleurs dans une interface web interactive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33d59bd",
   "metadata": {},
   "source": [
    "## 2. Chargement et aperçu des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2608539",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.11.9' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Chemin vers le fichier\n",
    "df = pd.read_csv(\"./train.csv\")\n",
    "\n",
    "# Aperçu des données\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc20e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colonnes de labels multilabel\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# Nombre de commentaires par classe\n",
    "df[labels].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a8c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valeurs manquantes :\", df.isnull().sum())\n",
    "print(\"Doublons :\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83605ad1",
   "metadata": {},
   "source": [
    "## 3. Analyse exploratoire (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55397abe",
   "metadata": {},
   "source": [
    "- Longueur des commentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a93d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_length'] = df['comment_text'].apply(len)\n",
    "\n",
    "# Statistiques\n",
    "df['text_length'].describe()\n",
    "\n",
    "# Distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df['text_length'], bins=100)\n",
    "plt.title(\"Distribution de la longueur des commentaires\")\n",
    "plt.xlabel(\"Longueur du texte\")\n",
    "plt.ylabel(\"Nombre de commentaires\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fed769",
   "metadata": {},
   "source": [
    "- Répartition des labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53065a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nb_labels'] = df[labels].sum(axis=1)\n",
    "\n",
    "df['nb_labels'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title(\"Nombre de commentaires par nombre de labels\")\n",
    "plt.xlabel(\"Nombre de labels\")\n",
    "plt.ylabel(\"Nombre de commentaires\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c637ae61",
   "metadata": {},
   "source": [
    "- Corrélations entre classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47befa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "corr = df[labels].corr()\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Corrélation entre les types de toxicité\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db0d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "for label in labels:\n",
    "    toxic_texts = \" \".join(df[df[label]==1][\"comment_text\"].astype(str))\n",
    "    wc = WordCloud(max_words=100, background_color='white').generate(toxic_texts)\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Wordcloud pour la classe : {label}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42364639",
   "metadata": {},
   "source": [
    "## 4. Nettoyage et prétraitement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac99e0",
   "metadata": {},
   "source": [
    "- Nettoyage texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270bd0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https/S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"[^/w/s]\", '', text)  # supprimer ponctuation\n",
    "    text = re.sub(r\"/d+\", \"\", text)      # supprimer chiffres\n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df['comment_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0631c3d6",
   "metadata": {},
   "source": [
    "- Transformation des labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a268df63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "X = df['clean_text']  # ou 'comment_text' si pas nettoyé\n",
    "y = df[labels]\n",
    "\n",
    "# Split pour entraînement/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7421ccbf",
   "metadata": {},
   "source": [
    "## 5. Modèle de base : TF-IDF + LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb438722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad30c114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Exemple de tokenisation (automatisable ensuite avec Dataset ou DataLoader)\n",
    "encoded_train = tokenizer(list(X_train), padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "encoded_test = tokenizer(list(X_test), padding=True, truncation=True, max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3742f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_labels = torch.tensor(y_train.values).float()\n",
    "test_labels = torch.tensor(y_test.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e156d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Chargement\n",
    "df = pd.read_csv(\"./train.csv\")\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# Nettoyage optionnel\n",
    "df['clean_text'] = df['comment_text'].str.lower()\n",
    "\n",
    "# Split\n",
    "X = df['clean_text']\n",
    "y = df[labels]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Modèle\n",
    "model = OneVsRestClassifier(LogisticRegression(solver='liblinear'))\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Prédictions\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Rapport\n",
    "print(\"F1-score micro :\", f1_score(y_test, y_pred, average='micro'))\n",
    "print(\"F1-score macro :\", f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "print(\"\\nClassification Report :\")\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f9d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Sauvegarde du modèle\n",
    "joblib.dump(model, \"model_tfidf_logreg.pkl\")\n",
    "\n",
    "# Sauvegarde du TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd4e908",
   "metadata": {},
   "source": [
    "## 6. Transfert Learning 1 : DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c53c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import Dataset\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv(\"./train.csv\")\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "df['labels'] = df[labels].astype(float).values.tolist()\n",
    "\n",
    "# Split\n",
    "train_df, test_df = train_test_split(df[['comment_text', 'labels']], test_size=0.2, random_state=42)\n",
    "\n",
    "# Dataset Hugging Face\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"comment_text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Modèle\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(labels),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(pred):\n",
    "    logits = torch.tensor(pred.predictions)\n",
    "    probs = torch.sigmoid(logits).numpy()\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    labels_ = pred.label_ids\n",
    "    return {\n",
    "        \"f1_micro\": f1_score(labels_, preds, average=\"micro\"),\n",
    "        \"f1_macro\": f1_score(labels_, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "# Arguments d'entraînement — compatibles avec transformers==4.5.2\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    fp16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=250\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Entraînement\n",
    "trainer.train()\n",
    "\n",
    "# Évaluation (à faire manuellement car pas de evaluation_strategy='epoch' dispo)\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70cea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(test_dataset)\n",
    "print(compute_metrics(preds))\n",
    "\n",
    "# Optionnel : rapport plus détaillé\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "probas = torch.sigmoid(torch.tensor(preds.predictions)).numpy()\n",
    "binary_preds = (probas >= 0.5).astype(int)\n",
    "\n",
    "print(classification_report(test_dataset[\"labels\"], binary_preds, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defcb346",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./bert_model\")\n",
    "tokenizer.save_pretrained(\"./bert_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21f1b84",
   "metadata": {},
   "source": [
    "## 7. Transfert Learning 2 : BERT base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8622b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Chargement\n",
    "df = pd.read_csv(\"./train.csv\")\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "df['labels'] = df[labels].astype(float).values.tolist()\n",
    "\n",
    "# Split\n",
    "train_df, test_df = train_test_split(df[['comment_text', 'labels']], test_size=0.2, random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"comment_text\"], padding=\"max_length\", truncation=True, max_length=190)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Modèle\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(labels),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "# Fonction de métriques\n",
    "def compute_metrics(pred):\n",
    "    preds = torch.sigmoid(torch.tensor(pred.predictions)).numpy()\n",
    "    preds = (preds >= 0.5).astype(int)\n",
    "    labels_ = pred.label_ids\n",
    "    return {\n",
    "        \"f1_micro\": f1_score(labels_, preds, average=\"micro\"),\n",
    "        \"f1_macro\": f1_score(labels_, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "# Entraînement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    fp16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77ac5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(test_dataset)\n",
    "print(compute_metrics(preds))\n",
    "# Optionnel : rapport plus détaillé\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "probas = torch.sigmoid(torch.tensor(preds.predictions)).numpy()\n",
    "binary_preds = (probas >= 0.5).astype(int)\n",
    "\n",
    "print(classification_report(test_dataset[\"labels\"], binary_preds, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003a2ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./distilbert_model\")\n",
    "tokenizer.save_pretrained(\"./distilbert_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a338c9",
   "metadata": {},
   "source": [
    "## 8. Comparaison des modèles\n",
    "\n",
    "Nous avons entraîné et évalué trois modèles de classification multilabel afin d’identifier les commentaires toxiques :\n",
    "\n",
    "| Modèle                       | F1-score Micro | F1-score Macro |\n",
    "|-----------------------------|----------------|----------------|\n",
    "| TF-IDF + LogisticRegression | 0.78           | 0.56           |\n",
    "| DistilBERT                  | 0.84           | 0.68           |\n",
    "| BERT base                   | 0.86           | 0.70           |\n",
    "\n",
    ">Ces résultats peuvent légèrement varier selon le nombre d’époques, la taille de l’échantillon, et la graine aléatoire utilisée.\n",
    "\n",
    "### Observations :\n",
    "- Le modèle **TF-IDF** est très rapide à entraîner, mais ses performances sont limitées, surtout sur les classes minoritaires.\n",
    "- **DistilBERT** offre un bon compromis entre performance et vitesse d’exécution.\n",
    "- **BERT base** est le plus performant sur les deux métriques, au prix d’un temps d’entraînement plus long.\n",
    "\n",
    "### Visualisation des scores F1 :\n",
    "\n",
    "Nous représentons ci-dessous les F1-scores micro et macro pour mieux visualiser l’écart de performance entre les trois modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c617d8ed",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "### Meilleur modèle\n",
    "\n",
    "Parmi les trois modèles testés, **BERT base** obtient les **meilleures performances** avec un F1-score micro de **0.86** et un F1-score macro de **0.70**. Il surpasse à la fois le modèle de base et DistilBERT, en particulier sur les classes minoritaires comme `threat` ou `identity_hate`.\n",
    "\n",
    "### Justification des choix\n",
    "\n",
    "- **TF-IDF + Logistic Regression** a été utilisé comme **modèle de référence**. Il est simple, rapide à entraîner, mais montre des limites dès qu’il s’agit de capturer la complexité du langage naturel.\n",
    "- **DistilBERT** a été sélectionné pour sa légèreté et sa rapidité. Il a offert un bon compromis pour une utilisation en temps réel dans l’application Streamlit.\n",
    "- **BERT base** a été conservé pour démontrer le potentiel du Transfert Learning sur un problème multilabel. Malgré un temps de traitement plus élevé, ses résultats justifient pleinement son intégration comme meilleur modèle.\n",
    "\n",
    "### Ce qu’on aurait pu améliorer\n",
    "\n",
    "- **Optimisation des hyperparamètres** : nous avons utilisé des valeurs par défaut ou simplifiées (2 époques, batch size fixe). Une recherche par grille ou via Optuna aurait permis d’améliorer les résultats.\n",
    "- **Gestion du déséquilibre des classes** : certaines classes comme `threat` sont très sous-représentées. Un rééchantillonnage ou un système de pondération des classes aurait pu améliorer leur détection.\n",
    "- **Augmentation des données** : non réalisée ici car peu pertinente avec les modèles préentraînés, mais elle aurait pu être testée sur le modèle TF-IDF.\n",
    "- **Explicabilité** : l’ajout d’outils d’explicabilité type SHAP ou LIME aurait permis d’analyser les décisions des modèles profonds.\n",
    "\n",
    "---\n",
    "\n",
    "Ce TP nous a permis de mettre en pratique l’analyse exploratoire, la modélisation de texte en NLP, le fine-tuning de modèles préentraînés, ainsi que le déploiement d’un outil de visualisation interactif avec Streamlit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a41b1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
